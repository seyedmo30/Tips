# interview postgres

همیشه ۲ تا مسعله پرسیده میشه و من نمی تونم جواب بدم

## اگر سرعت کوییری ها پایین بیاد باید چیکار کنیم؟

### explain analyze


### vacuum


### Enabling Logging

میتونیم با تغییر این فایل لاگ رو فعال کنیم : `postgresql.conf`

```
log_min_duration_statement = 1000  # Log queries taking longer than 1000 ms (1 second)
log_statement = 'all'               # Log all statements (optional)
log_directory = 'pg_log'            # Directory for log files
log_filename = 'postgresql-%Y-%m-%d_%H%M%S.log'  # Log file naming pattern

```
و لاگ رو ببینیم 

`tail -f /path/to/your/pg_log/postgresql-*.log`


### pg_stat_activity

با استفاده از این میتونیم ببینیم کدام کلاینت کانکت شده و چه کاری انجام میده و چه کوییری هایی زده و چقد زمان برده اطلاعاتی مانند ip و یوزرنیم و ... کار بر توش هست

```sql

SELECT pid, usename, state, query, now() - query_start AS duration
FROM pg_stat_activity
WHERE state = 'active'
ORDER BY duration DESC;
```

SELECT * FROM orders WHERE status = 'pending';      ------- 00:00:45
UPDATE products SET stock = stock - 1 WHERE id = 1001;   -- 00:00:30


### Monitoring Logs
می تونیم لاگ هایی که مورد نظرمونه رو نمایش بدیم و نگه داریم

با استفاده از تولز هایی این کار رو انجام میدیم
`postgres_exporter`

### pg_stat_statements
ابتدا باید فعالش کنیم و ببینیم هر کوییری چقدر هزینه بر بوده :

`CREATE EXTENSION pg_stat_statements;`


```sql
SELECT
    query,
    calls,
    total_time,
    rows,
    mean_time,
    stddev_time
FROM
    pg_stat_statements
ORDER BY
    total_time DESC
LIMIT 10;
```

پاسخ این کوییری این است که بیشترین کوییری ها که تکرار شدن ، چقد زمان بردن


### work_mem

ظرفیت حافظه کاری که برای هر کوییری استفاده می شود و داخل آن ریخته میشود

در حقیقت هر بار که کوییری میزنیم کلی داده تمییز نشده که بر اساس sort , where , groupby  جدا جدا در مموری میاد و میتونیم سقف ظرفیت رو بالا ببریم

`SET work_mem = '16MB';`

### shared_buffers

این حافظه ی `cache` است نتایج اخیر رو ذخیره میکنه  به صورت پیش فرض 128 MB است و میتونیم آن را بالا ببریم

`SHOW shared_buffers;`

SET shared_buffers = '128MB';`
### pg_locks

با استفاده از این دستور میتونیم ببینیم که کدام لاک رو استفاده میکنه و چه حالتی داره

همچنین با تنظیم انواع ترن اکشن ها می تونیم با توجه به بیزینس ، سرعتو بهبود ببخشیم :



**Access Share Lock**
SELECT

**Row Exclusive Lock**

INSERT, UPDATE, or DELETE

**Share Lock**

SELECT FOR UPDATE


```sql

SELECT
    pid,
    mode,
    relation::regclass AS relation,
    transactionid,
    virtualtransaction,
    state,
    granted
FROM
    pg_locks
WHERE
    NOT granted;


```

## نوشتن کوییری های پیشرفته

به نظرم به هوش مصنوعی بگیم سوال بپرسه بهتره ولی حتما grouup by ها رو بگیم , join , having 

مثال

```sql

SELECT
    c.name,
    SUM(o.total_amount) AS total_spend
FROM
    customers c
JOIN
    orders o ON c.customer_id = o.customer_id
GROUP BY
    c.customer_id, c.name
HAVING
    SUM(o.total_amount) > 500
ORDER BY
    total_spend DESC;

```
## ادامه

#اون طور که فهمیدم کسی که از لوکال به پستگرس وصل میشه نیازی به پسورد نداره

چون از **UDS Unix domain socket** استفاده می کنه

configure

برای پیکر بندی بعد از نصب باید با دستور زیر ون رو استارت کرد

pg_ctlcluster 14 main start


پس از این باید بایوزر دیفالت وارد شد

sudo -i -u postgres

دستور بالا را فقط باید تو ssh یا تو لوکال زد ، و یک دستور سیستم عاملی هست ،  هست ، -i یعنی منتظر بمون و -u postgres یعنی با این یوزر ، دقت شود تا اینجا هیچ ربطی به پستگرس ندارد و این یوزر داخل فایل زیر هست

less /etc/passwd  





و برای این که وارد شویم باید در آخرش psql  گذاشت

سپس رمز برای یوزر پستگرس ست می کنیم

ALTER USER postgres PASSWORD 'salam';


بعد می توانیم برای ریموت زدن ، فایل ها رو تنظیم کنیم

/etc/postgresql/14/main/pg_hba.conf


host all all 0.0.0.0/0 md5


/etc/postgresql/14/main/postgresql.conf


listen_addresses = '*'



listen_addresses = 'localhost,192.168.13.14'


یکی از راه های ساختن تیبل در اینیت کردن دیتا بیس اینه که DDL ها رو توی فایل ریخته و با دستور زیر فایل اجرا شود :

    psql -U postgres -d news_fetcher -h localhost -p 5432 -a -f init-sql.sql

در فایل init-sql.sql می توان مشخصات تیبل ها رو ریخت



و در نهایت با دستور زیر ریموت بزنیم

psql -h 192.168.13.14 -p 5432 -d postgres -U postgres


secure


https://www.atlantic.net/vps-hosting/how-to-secure-postgresql-server/



برای ریموت باید ابتدا یوزر پسورد رو توی pgpass گذاشت برای راحتی و استفاده از کرون تب   

role vs user 


توی پست گرس رول داریم
که به اون پرمیشن می دیم
به صورت پیش فرض رولی که پرمیشن لاگین رو داره ، بهش یوزر میگیم 




ورود به پستگرس
اگر دستور psql رو در ترمینال بزنیم . پستگرس به صورت پیشفرض یوزر رو یوزر os و دیتا بیس رو پیشفرض میگیره و خطا میده چون ایجاد نکردیم



VIEW


با استفاده از ویو ، می تونیم به جای سلکت های تکراری و پیچیده از این فیچر استفاده کنیم و ب ای هر ویو اسمی انتخاب کنیم و بجای سلکت کردن ، ویو رو سلکت کنیم 


MATETIALIZED VIEW 


با مفهوم بالا می تونیم ویو رو تعمیم بدیم، به این گونه که اگر سلکت هایی که نیاز به پردازش زیاد داره و می خوایم تو روز تعداد محدودی بروز بشوند از این استفاده کنیم
بعد از هر بار رفرش کردن متریالازر داده ها ثابت می مونن( مانند تیبل های موقت )


sequence


مانند آتو اینکیریمنت هست یعنی با یک سیکونس داده تولید می کنه 

aggregate function


می تونیم اگریگیت های کایتوم تولید کنیم 

cte with


یه جورایی شبیه تیبل تمپراری هست
می شه یه سلکت یا آپدیت رو بهش اسم داد و بعدا از اون استفاده کرد
مثلا  دو کوییری رو با with اسم گزاری می کنیم و در کوییری سوم آن هارا استفاده می کنیم



with recursive


می توان به یک کوییری اسم داد و با روش باز گشتی ، داده را فچ کرد
بهترین روش برای پیمایش درخت


tablespace


با استفاده از این ، مشخص میکنیم مکان ذخیره در حافظه کجا باشد.  زمانی استفاده میشود که یا پارتیشن پر شده و بخواهیم بقیه داده را در مکانی دیگر ذخیره کنیم ، یا این که داده های کم اهمییت تر را در جایی دیگر ذخیره کنیم

upsert - on conflict
در شرایطی که اینسرت با مشکل مواجه می شود ، از تکنیک هایی استفاده می کنیم که راه حل جایگزین استفاده شود 

cursor

گاهی داده ای که نیاز است فچ کنیم ، تعداد سطر های زیادی دارد و در صورت دریافت همه ی آنها ، مموری الکی اشغال می شه ، می توانیم یک ترنزاکشن باز کنیم و یک شی از داده ی سلکت شده در آن بسازیم و تکه تکه ، از کرسر داده را بخوانیم ، در حقیقت پیمایش یا ایتریت کردن داده سنگین از دیتا بیس را این مفهوم هندل می کند . نکته : در جوانی بعد از ایجاد کرسر ، آن رو فچ آل می کردیم و خب از این قابلیت استفاده نمی کردیم .


### lookup table
اگر بخواهیم ستونی با تنوع داده های محدود داشته باشیم مثلا ستون زبان و داده هایی مانند انگیسی و فارسی و... 

حال اگر تنوع داده ثابت باشد می توان از ENUM or smallint استفاده کرد اما اگر امکان افزایش باشد ۲ روش مجود دارد .

+ راه direct varible (varchar ) ـــ در این روش به صورت تکراری مقدار را می نویسیم ایرا اینه readandancy یا تکراری بودن پیش میاد همچنین aggregate مشکل هست



+ راه lookup table (reference table) ــــ این همون جدول فارن کی هست خوبیش اینه از تکراری بودن داده جلوگیری می کنه همچنین اجازه ورود هر داده را نداره بدیش اینه جویین نیازه ولی اگر بخوهیم از groupby aggregate استفاده کنیم بهتر است  

### lookup table vs varchar index

گاهی یه کالمن زیاد داده هاش متمایز نیست cardinality (number of distinct values) و تعدادشون کمه ، در این صورت بهتره تنها ایندکس کنیم

اما اگر تمایز زیاد بود یا احتمال می دادیم که در آینده پییدگی قراره اضافه بشه یا طول ورچر خیلی قراره طولانی بشه بهتر از لوکاپ تیبل استفاده کنیم

همچنین لوکاپ می تواند راحت تر کنترل کند داده ی جدید تکراری است یا جدید

در کل نتیجه ای که گرفتم ، اگر قراره کاربر بیاد و دستی اضافه کنه ، لوکاء بهتره ولی اگر مانند enum تعداد مشخصه ، ایندکس گذاری هم در پرفورمنس هم در راحتی و خوانایی بهتره

### index

هر موقع ما یه ستون را ایندکس می کنیم ، در حقیقت یه تیبل جدید می سازیم که با توجه به نوع ایندکس ، ترتیب آن چیده میشود

مثل اگر یک جدول ، ۳ ایندکس دارد ، ۳ تیبل جدا وجود دارد که با توجه به سیاست ما ، ان مرتب شده

مثلن اگر سرچ کنیم دانشجو های مرد و متاهل و 20 نفر ، ابتدا پستگرس با استفاده از planner  تصمیم می گیرد از ایندکس مرد ها چند دانشجو بردارد و آن ها را در ایندکس متاهل ها تا ۲۰ عدد بردارد   

نکته منفی این است که هر تیبل به صورت موازی چند داد می آورد و در نهایت اشتراک گیری می شود که زمان بر است ، راه حل پایین :

می توان از کامپوزیت ایندکس  استفاده کرد ، یعنی چند ستون را یک ایندکس کنیم 

با توجه به این که میدانیم کوییری بالا خیلی استفاده می شود  ، هر دو را در یک ایندکس می گذاریم .

توجه داشته باشید ترتیب کامپوزیت ایندکس در تایپ B_TREE  خیلی مهم است و شرط های اصلی باید سمت چپ باشند

همچنین می توان ابتدای کوییری explain analize گذاشت و دید که زمان و ترتیب جست و جو در ایندکس ها چقدر است

**tips**

دلیل این نام گذاری ، عمق( depth ) این درخت ها ثابت است 
 مثلا استفاده از اینکس با تایپ B_Tree (Balanced tree)  ، **پیچیدگی زمانی** در این ایندکس برابر **O(log n)** است  

**مثال**

```sql
CREATE TABLE Orders (
    OrderID INT PRIMARY KEY,
    CustomerID INT,
    OrderDate DATE,
    Amount DECIMAL(10, 2)
);

CREATE INDEX idx_customer_orderdate ON Orders(CustomerID, OrderDate);

INSERT INTO Orders (OrderID, CustomerID, OrderDate, Amount) VALUES
(1, 101, '2024-01-15', 250.00),
(2, 102, '2024-01-16', 150.00),
(3, 101, '2024-01-17', 200.00),
(4, 103, '2024-01-18', 300.00),
(5, 102, '2024-01-19', 350.00),
(6, 101, '2024-01-20', 400.00);


-- Query using the composite index
SELECT * FROM Orders 
WHERE CustomerID = 101 AND OrderDate >= '2024-01-15';


              [101, 102, 103]
             /        |        \
          /           |         \
   [101, 2024-01-15] [102, 2024-01-16] [103, 2024-01-18]
      /      |       \                \
     /       |        \                \
[101, 2024-01-17]  [102, 2024-01-19]  [NULL]
      |
  [101, 2024-01-20]


```

همچنین توجه شود اگر ستونی که تنها ۳ داده مانند "good", "normal", and "bad"  راذخیره می کند ، ایندکس کنیم ، تکرار در ذخیره ایجاد نمی شود


###  tips
+ تعداد کانکشن به صورت دیفالت ۱۰۰ تا هست و می توان با این دستور لیست و تعداد سرویس هایی که کانت شده اند را دید .
  
      SELECT * FROM pg_stat_activity;
  + باید بعد از هر کانکشن در سرویس، کلوز کرد .
  + همیشه در سینگلتون کانکشن ، می بایست ریکانکشن هم نوشت . 

###  **Preper statement**

به صورت دیفالت هر بار که درخواست ثبت در دیتا بیس میزنیم تمامی اطلاعات مانند insert into  ارسال میشه ، میتونیم با ننظیم این مفهوم ، دیگه این رو ارسال نکنیم و سرعت رو بیشتر کنیم



## transaction

### tips

**Skip default transaction**

میتونیم موقع کانکشن بگیم نیاز نیست برای هر کوییری ، ترنساکشن بسازی و سرعت بیشتر میشه

