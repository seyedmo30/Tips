# interview postgres

همیشه ۲ تا مسعله پرسیده میشه و من نمی تونم جواب بدم

+ اگر سرعت کوییری ها پایین بیاد باید چیکار کنیم؟

+ advance query (select , groupby,having,join,subquery)

## اگر سرعت کوییری ها پایین بیاد باید چیکار کنیم؟

یه اصطلاحی هست تحت عنوان  tuning هست که می تونه به سرعت کوییری ها کمک کنه

### explain analyze

اگر یه کوییری ثابت اذییت کرد و سنگین شد از explain analize  استفاده میکنیم به این صورت که اول کوییری این دستور رو مینویسیم


مثلن می تونیم با استفاده از ایندکس ها و دوباره بررسی از طریق explain analyze سرعت کوییری رو محاسبه کنیم و با استفاده از تست ها ، tuning کنیم


```sql

EXPLAIN SELECT * FROM employees
WHERE department_id = 10
ORDER BY salary DESC;

```


و پاسخ

```sql
                         Query Plan                         
-------------------------------------------------------------
 Seq Scan on employees  (cost=0.00..45.25 rows=10 width=220)
   Filter: (department_id = 10)
   ->  Sort  (cost=0.00..0.00 rows=0 width=220)
         Sort Key: salary DESC

```

در این صورت تنها cost رو داریم اما اگر بخواهیم علاوه بر پلن ، زمان کلی رو بدست بیاریم ، باید آنالیز هم کنیم 

```sql
EXPLAIN ANALYZE SELECT * FROM employees
WHERE department_id = 10
ORDER BY salary DESC;

```

باید آنایز افزوده شود

```sql
                                                               Query Plan                                                                
----------------------------------------------------------------------------------------------------------------------------------------
 Sort  (cost=160.00..165.00 rows=10 width=220) (actual time=3.345..3.737 rows=8 loops=1)
   Sort Key: salary DESC
   Sort Method: quicksort  Memory: 25kB
   ->  Seq Scan on employees  (cost=0.00..45.25 rows=10 width=220) (actual time=0.018..3.061 rows=8 loops=1)
         Filter: (department_id = 10)
         Rows Removed by Filter: 992
 Planning Time: 0.364 ms
 Execution Time: 3.789 ms
(7 rows)
```

میشه متوجه شد که میره ۱۰۰۰ تا سطر رو میبینه ، چون ایندکس idx_department_id   نداره پس این ایندکس رو اضافه میکنیم تا مستقیم تنها رو ۸ تا مورد سرچ بزنه


```sql
CREATE INDEX idx_department_id ON employees(department_id); 
```

```sql
                                                               Query Plan                                                                
----------------------------------------------------------------------------------------------------------------------------------------
 Sort  (cost=160.00..165.00 rows=10 width=220) (actual time=3.345..3.737 rows=8 loops=1)
   Sort Key: salary DESC
   Sort Method: quicksort  Memory: 25kB
   ->  Seq Scan on employees  (cost=0.00..45.25 rows=10 width=220) (actual time=0.018..3.061 rows=8 loops=1)
         Filter: (department_id = 10)
         Rows Removed by Filter: 992
 Planning Time: 0.364 ms
 Execution Time: 3.789 ms
(7 rows)
```

### vacuum

در پستگرس (PostgreSQL)، دستور VACUUM به منظور پاکسازی و بهینه‌سازی دیتابیس استفاده می‌شود. وقتی داده‌ها در پایگاه‌داده حذف یا به‌روزرسانی می‌شوند، فضای حافظه‌ای که اشغال کرده‌اند به‌طور خودکار آزاد نمی‌شود. این موضوع باعث می‌شود که پایگاه‌داده به مرور زمان حجیم‌تر شود و کارایی آن کاهش یابد. دستور VACUUM به این کمک می‌کند که این فضای آزاد شده را دوباره بازیابی کرده و عملکرد سیستم را بهبود بخشد.

وقتی که رکوردی در جدول حذف یا به‌روزرسانی می‌شود، پستگرس به جای حذف فیزیکی رکورد از دیسک، آن را علامت‌گذاری به عنوان "حذف شده" می‌کند

همچنین میشه پلنر رو آپدیت کرد ، شاید داده ها قدیمی شدن یا پاک شدن پس نیازه که با وکییوم ، بروز بشن


**tip** 

با این که **autovacuum** به صورت خود کار اجرا میشه ، ولی چرا باز این دستور دستی میشه زد ؟

شاید جداول خیلی بزرگ رو نتونه autovacuum به صورت کامل اجرا کنه
 ، همچنین گاهی چون این درخواست زمان زیادی می برد خود پستگرس اتوماتیک انجام نمی دهد


**Tuple**

در حقیقت همان سطر های موجود در تیبل هستند و چند نوع دارند

+ **Dead tuple**

تاپل هایی که اکتیو نیستن و توی کوییری ها نمایش داده نمیشه اما هنوز پاک نشدن ، اینا یا  سطر قبل update بودن یا delete شدن اما به هر ترتیب هستند و نیاز به پاک سازی دارن

+ **Live tuple**

اینا واقعن سطر های موجود و درست هستند



`VACUUM ANALYZE employees;`

**ANALYZE**: آمار و اطلاعات مربوط به جدول employees را به‌روزرسانی می‌کند.


می تونیم بعد از این درخواست نتیجه رو ببینیم

`SELECT * FROM pg_stat_user_tables WHERE relname = 'employees';`

با به‌روزرسانی آمار، بهینه‌ساز (Query Planner) قادر خواهد بود که بهترین برنامه‌های اجرایی را برای کوئری‌ها انتخاب کند.

نتیجه وکیووم


 schemaname |   relname   | seq_scan | seq_tup_read | idx_scan | idx_tup_fetch | n_tup_ins | n_tup_upd | n_tup_del | n_tup_hot | n_live_tup | n_dead_tup | last_vacuum | last_autovacuum | last_analyze | last_autoanalyze | vacuum_count | analyze_count 
-------------+-------------+----------+---------------+----------+----------------+-----------+-----------+-----------+-----------+-------------+-------------+--------------+-----------------+--------------+-------------------+--------------+----------------
 public      | employees   | 50       |        50000 |     1000 |            800 |      2000 |      1000 |      500  |      300  |       48000 |        2000 | 2023-11-01   |                 | 2023-11-01   |                   |            2 |              3



**query planner**

یه سری اطلاعات مانند تعداد سطر ها ، تعدادایندکس ها  ، با استفاده از این آمار تصمیم می گیره برای مثال اگر تعداد جدول کم باشد تصمیم میگیرد به جای رفتن سراغ جدول ایندکس و جویین زدن ، داخل همون جدول سرچ کند (نادیده گرفتن ایندکس) اما اگر آمار رو بروز کنیم شاید بره از ایندکس داده رو بیاره شاید تعداد تاپل ها زیاد شده اما در آمار کمه


### Enabling Logging

میتونیم با تغییر این فایل لاگ رو فعال کنیم : `postgresql.conf`

```
log_min_duration_statement = 1000  # Log queries taking longer than 1000 ms (1 second)
log_statement = 'all'               # Log all statements (optional)
log_directory = 'pg_log'            # Directory for log files
log_filename = 'postgresql-%Y-%m-%d_%H%M%S.log'  # Log file naming pattern

```
و لاگ رو ببینیم 

`tail -f /path/to/your/pg_log/postgresql-*.log`

بعد از فعال کردن لاگ ، این طور میبینیم :


+ همچنین میشه لاگ ها رو آپشنال کرد و مشاهده کرد ، در مثال زیر کوییری هایی که خیلی سرعت کمی دارند رو میشه مشاهده کرد

SET log_min_duration_statement = 1000; -- log queries taking longer than 1000 ms



```sql
2024-10-19 09:02:30.789 UTC [12347] LOG:  duration: 10 ms  statement: INSERT INTO employees (name, department, salary) VALUES ('John Doe', 'Sales', 50000);

2024-10-19 09:01:10.456 UTC [12346] LOG:  duration: 2000 ms  statement: UPDATE employees SET salary = salary * 1.1 WHERE department = 'Marketing';

2024-10-19 09:00:00.123 UTC [12345] LOG:  duration: 1500 ms  statement: SELECT * FROM employees WHERE department = 'Sales';

2024-10-19 09:06:30.123 UTC [12351] LOG:  statement: BEGIN;

2024-10-19 09:06:35.456 UTC [12351] LOG:  statement: COMMIT;

2024-10-19 09:07:00.789 UTC [12352] ERROR:  relation "non_existent_table" does not exist
2024-10-19 09:07:00.789 UTC [12352] STATEMENT:  SELECT * FROM non_existent_table;


```
### pg_stat_activity

با استفاده از این میتونیم ببینیم کدام کلاینت کانکت شده و چه کاری انجام میده و چه کوییری هایی زده و چقد زمان برده اطلاعاتی مانند ip و یوزرنیم و ... کار بر توش هست

```sql

SELECT pid, usename, state, query, now() - query_start AS duration
FROM pg_stat_activity
WHERE state = 'active'
ORDER BY duration DESC;
```

SELECT * FROM orders WHERE status = 'pending';      ------- 00:00:45
UPDATE products SET stock = stock - 1 WHERE id = 1001;   -- 00:00:30


### Monitoring Logs
می تونیم لاگ هایی که مورد نظرمونه رو نمایش بدیم و نگه داریم

با استفاده از تولز هایی این کار رو انجام میدیم
`postgres_exporter`

### pg_stat_statements
ابتدا باید فعالش کنیم و ببینیم هر کوییری چقدر هزینه بر بوده :

`CREATE EXTENSION pg_stat_statements;`


```sql
SELECT
    query,
    calls,
    total_time,
    rows,
    mean_time,
    stddev_time
FROM
    pg_stat_statements
ORDER BY
    total_time DESC
LIMIT 10;
```

پاسخ این کوییری این است که بیشترین کوییری ها که تکرار شدن ، چقد زمان بردن


### work_mem

ظرفیت حافظه کاری که برای هر کوییری استفاده می شود و داخل آن ریخته میشود

در حقیقت هر بار که کوییری میزنیم کلی داده تمییز نشده که بر اساس sort , where , groupby  جدا جدا در مموری میاد و میتونیم سقف ظرفیت رو بالا ببریم

`SET work_mem = '16MB';`

### shared_buffers

این حافظه ی `cache` است نتایج اخیر رو ذخیره میکنه  به صورت پیش فرض 128 MB است و میتونیم آن را بالا ببریم

`SHOW shared_buffers;`

SET shared_buffers = '128MB';`
### pg_locks

با استفاده از این دستور میتونیم ببینیم که کدام لاک رو استفاده میکنه و چه حالتی داره

همچنین با تنظیم انواع ترن اکشن ها می تونیم با توجه به بیزینس ، سرعتو بهبود ببخشیم :



**Access Share Lock**
SELECT

**Row Exclusive Lock**

INSERT, UPDATE, or DELETE

**Share Lock**

SELECT FOR UPDATE


```sql

SELECT
    pid,
    mode,
    relation::regclass AS relation,
    transactionid,
    virtualtransaction,
    state,
    granted
FROM
    pg_locks
WHERE
    NOT granted;


```



## سرعت نوشتن (write) کند شده ، چه کا کنیم ؟

#### بررسی ایندکس ها 

همون طور که سرعت خوندن رو زیاد می کنه اگر داده ها زیاد شه و ایندکس ها  زیاد شه ، شاید برای هر سطر باید چندین جدول ایندکس ، بهش سطر اضافه بشه

#### Batch Insert

#### WAL level

 از WAL level مناسب استفاده کنید و تنظیمات checkpoint را بهینه کنید.


#### sharding

#### transaction isolation level


#### synchronous_commit = OFF
 میتونیم با  fsync and synchronous_commit   بک آپ برای کرش یا سوختگی رو خاموش کنیم ، اما خطرناکه

Set synchronous_commit = OFF;  -- Not recommended without understanding the implications

### همچنین اگه سرعت نوشتن اومده باشه پایین باید چی کار کرد؟

+ بررسی کنیم i/o سخت افزاری رو ابتدا ببینیم هارد دیسک کند نشده ؟ و اینکه شاید از سمت سیستم عامل و در کل ربطی به پایگاه داده نیست

+ شاید WAL رو خوب مدیریت نکردیم و همه چیز رو داره ارشیو می کنه و پاک نمی کنه و بررسی کانفیگ archive_mode and archive_command

+ شاید  **Table Bloat**  رخ داده ، یعنی بر اثر تغییر های زیاد مانند آپدیت و دیلیت مدیریت حافظه از بهینه در اومده و با استفاده **VACUUM FULL** میشه بهبود دد البته توجه کنید این کار تیبل رو لاک می کنه .  توضیح در باره ی **Table bloat** این پدیده زمانی می گیم که حافظه ی تخصیص داده شده برای ذخیره بیشتر از مقدار مورد نیاز برای ذخیره سازی داده ها استفاده می شود. و زمانی رخ میدهد که تعداد تغییرات تاپل ها زیاد است ، دلایل زیر می توان باعث این دیده شه : 
 MVCC (Multi-Version Concurrency Control) و Frequent Updates and Deletes 


+ بررسی **Lock Contention** با استفاده از `pg_stat_activity`  و همچنین با استفاده از `pg_locks` میتونیم ببینیم که کدام لاک رو استفاده میکنه و چه حالتی داره

+ بررسی ایندکس ها اینکه همه چیز رو الکی ایندکس کردیم همچنین نگاه به کانفیگ shared_buffers, work_mem, maintenance_work_mem

+ اگه همه چیز اوکی بود باید با افزایش مموری یا ssd و تکنیک های شاردینگ درستش کنیم

## نوشتن کوییری های پیشرفته

به نظرم به هوش مصنوعی بگیم سوال بپرسه بهتره ولی حتما grouup by ها رو بگیم , join , having 

مثال

```sql

SELECT
    c.name,
    SUM(o.total_amount) AS total_spend
FROM
    customers c
JOIN
    orders o ON c.customer_id = o.customer_id
GROUP BY
    c.customer_id, c.name
HAVING
    SUM(o.total_amount) > 500
ORDER BY
    total_spend DESC;

```
## ادامه

#اون طور که فهمیدم کسی که از لوکال به پستگرس وصل میشه نیازی به پسورد نداره

چون از **UDS Unix domain socket** استفاده می کنه

configure

برای پیکر بندی بعد از نصب باید با دستور زیر ون رو استارت کرد

pg_ctlcluster 14 main start


پس از این باید بایوزر دیفالت وارد شد

sudo -i -u postgres

دستور بالا را فقط باید تو ssh یا تو لوکال زد ، و یک دستور سیستم عاملی هست ،  هست ، -i یعنی منتظر بمون و -u postgres یعنی با این یوزر ، دقت شود تا اینجا هیچ ربطی به پستگرس ندارد و این یوزر داخل فایل زیر هست

less /etc/passwd  





و برای این که وارد شویم باید در آخرش psql  گذاشت

سپس رمز برای یوزر پستگرس ست می کنیم

ALTER USER postgres PASSWORD 'salam';


بعد می توانیم برای ریموت زدن ، فایل ها رو تنظیم کنیم

/etc/postgresql/14/main/pg_hba.conf


host all all 0.0.0.0/0 md5


/etc/postgresql/14/main/postgresql.conf


listen_addresses = '*'



listen_addresses = 'localhost,192.168.13.14'


یکی از راه های ساختن تیبل در اینیت کردن دیتا بیس اینه که DDL ها رو توی فایل ریخته و با دستور زیر فایل اجرا شود :

    psql -U postgres -d news_fetcher -h localhost -p 5432 -a -f init-sql.sql

در فایل init-sql.sql می توان مشخصات تیبل ها رو ریخت



و در نهایت با دستور زیر ریموت بزنیم

psql -h 192.168.13.14 -p 5432 -d postgres -U postgres


secure


https://www.atlantic.net/vps-hosting/how-to-secure-postgresql-server/



برای ریموت باید ابتدا یوزر پسورد رو توی pgpass گذاشت برای راحتی و استفاده از کرون تب   

role vs user 


توی پست گرس رول داریم
که به اون پرمیشن می دیم
به صورت پیش فرض رولی که پرمیشن لاگین رو داره ، بهش یوزر میگیم 




ورود به پستگرس
اگر دستور psql رو در ترمینال بزنیم . پستگرس به صورت پیشفرض یوزر رو یوزر os و دیتا بیس رو پیشفرض میگیره و خطا میده چون ایجاد نکردیم



VIEW


با استفاده از ویو ، می تونیم به جای سلکت های تکراری و پیچیده از این فیچر استفاده کنیم و ب ای هر ویو اسمی انتخاب کنیم و بجای سلکت کردن ، ویو رو سلکت کنیم 


MATETIALIZED VIEW 


با مفهوم بالا می تونیم ویو رو تعمیم بدیم، به این گونه که اگر سلکت هایی که نیاز به پردازش زیاد داره و می خوایم تو روز تعداد محدودی بروز بشوند از این استفاده کنیم
بعد از هر بار رفرش کردن متریالازر داده ها ثابت می مونن( مانند تیبل های موقت )


sequence


مانند آتو اینکیریمنت هست یعنی با یک سیکونس داده تولید می کنه 

aggregate function


می تونیم اگریگیت های کایتوم تولید کنیم 

cte with


یه جورایی شبیه تیبل تمپراری هست
می شه یه سلکت یا آپدیت رو بهش اسم داد و بعدا از اون استفاده کرد
مثلا  دو کوییری رو با with اسم گزاری می کنیم و در کوییری سوم آن هارا استفاده می کنیم



with recursive


می توان به یک کوییری اسم داد و با روش باز گشتی ، داده را فچ کرد
بهترین روش برای پیمایش درخت


tablespace


با استفاده از این ، مشخص میکنیم مکان ذخیره در حافظه کجا باشد.  زمانی استفاده میشود که یا پارتیشن پر شده و بخواهیم بقیه داده را در مکانی دیگر ذخیره کنیم ، یا این که داده های کم اهمییت تر را در جایی دیگر ذخیره کنیم

upsert - on conflict
در شرایطی که اینسرت با مشکل مواجه می شود ، از تکنیک هایی استفاده می کنیم که راه حل جایگزین استفاده شود 

cursor

گاهی داده ای که نیاز است فچ کنیم ، تعداد سطر های زیادی دارد و در صورت دریافت همه ی آنها ، مموری الکی اشغال می شه ، می توانیم یک ترنزاکشن باز کنیم و یک شی از داده ی سلکت شده در آن بسازیم و تکه تکه ، از کرسر داده را بخوانیم ، در حقیقت پیمایش یا ایتریت کردن داده سنگین از دیتا بیس را این مفهوم هندل می کند . نکته : در جوانی بعد از ایجاد کرسر ، آن رو فچ آل می کردیم و خب از این قابلیت استفاده نمی کردیم .


### lookup table
اگر بخواهیم ستونی با تنوع داده های محدود داشته باشیم مثلا ستون زبان و داده هایی مانند انگیسی و فارسی و... 

حال اگر تنوع داده ثابت باشد می توان از ENUM or smallint استفاده کرد اما اگر امکان افزایش باشد ۲ روش مجود دارد .

+ راه direct varible (varchar ) ـــ در این روش به صورت تکراری مقدار را می نویسیم ایرا اینه readandancy یا تکراری بودن پیش میاد همچنین aggregate مشکل هست



+ راه lookup table (reference table) ــــ این همون جدول فارن کی هست خوبیش اینه از تکراری بودن داده جلوگیری می کنه همچنین اجازه ورود هر داده را نداره بدیش اینه جویین نیازه ولی اگر بخوهیم از groupby aggregate استفاده کنیم بهتر است  

### lookup table vs varchar index

گاهی یه کالمن زیاد داده هاش متمایز نیست cardinality (number of distinct values) و تعدادشون کمه ، در این صورت بهتره تنها ایندکس کنیم

اما اگر تمایز زیاد بود یا احتمال می دادیم که در آینده پییدگی قراره اضافه بشه یا طول ورچر خیلی قراره طولانی بشه بهتر از لوکاپ تیبل استفاده کنیم

همچنین لوکاپ می تواند راحت تر کنترل کند داده ی جدید تکراری است یا جدید

در کل نتیجه ای که گرفتم ، اگر قراره کاربر بیاد و دستی اضافه کنه ، لوکاء بهتره ولی اگر مانند enum تعداد مشخصه ، ایندکس گذاری هم در پرفورمنس هم در راحتی و خوانایی بهتره

### index

هر موقع ما یه ستون را ایندکس می کنیم ، در حقیقت یه تیبل جدید می سازیم که با توجه به نوع ایندکس ، ترتیب آن چیده میشود

مثل اگر یک جدول ، ۳ ایندکس دارد ، ۳ تیبل جدا وجود دارد که با توجه به سیاست ما ، ان مرتب شده

مثلن اگر سرچ کنیم دانشجو های مرد و متاهل و 20 نفر ، ابتدا پستگرس با استفاده از planner  تصمیم می گیرد از ایندکس مرد ها چند دانشجو بردارد و آن ها را در ایندکس متاهل ها تا ۲۰ عدد بردارد   

نکته منفی این است که هر تیبل به صورت موازی چند داد می آورد و در نهایت اشتراک گیری می شود که زمان بر است ، راه حل پایین :

می توان از کامپوزیت ایندکس  استفاده کرد ، یعنی چند ستون را یک ایندکس کنیم 

با توجه به این که میدانیم کوییری بالا خیلی استفاده می شود  ، هر دو را در یک ایندکس می گذاریم .

توجه داشته باشید ترتیب کامپوزیت ایندکس در تایپ B_TREE  خیلی مهم است و شرط های اصلی باید سمت چپ باشند

همچنین می توان ابتدای کوییری explain analize گذاشت و دید که زمان و ترتیب جست و جو در ایندکس ها چقدر است

**tips**

دلیل این نام گذاری ، عمق( depth ) این درخت ها ثابت است 
 مثلا استفاده از اینکس با تایپ B_Tree (Balanced tree)  ، **پیچیدگی زمانی** در این ایندکس برابر **O(log n)** است  

**مثال**

```sql
CREATE TABLE Orders (
    OrderID INT PRIMARY KEY,
    CustomerID INT,
    OrderDate DATE,
    Amount DECIMAL(10, 2)
);

CREATE INDEX idx_customer_orderdate ON Orders(CustomerID, OrderDate);

INSERT INTO Orders (OrderID, CustomerID, OrderDate, Amount) VALUES
(1, 101, '2024-01-15', 250.00),
(2, 102, '2024-01-16', 150.00),
(3, 101, '2024-01-17', 200.00),
(4, 103, '2024-01-18', 300.00),
(5, 102, '2024-01-19', 350.00),
(6, 101, '2024-01-20', 400.00);


-- Query using the composite index
SELECT * FROM Orders 
WHERE CustomerID = 101 AND OrderDate >= '2024-01-15';


              [101, 102, 103]
             /        |        \
          /           |         \
   [101, 2024-01-15] [102, 2024-01-16] [103, 2024-01-18]
      /      |       \                \
     /       |        \                \
[101, 2024-01-17]  [102, 2024-01-19]  [NULL]
      |
  [101, 2024-01-20]


```

همچنین توجه شود اگر ستونی که تنها ۳ داده مانند "good", "normal", and "bad"  راذخیره می کند ، ایندکس کنیم ، تکرار در ذخیره ایجاد نمی شود


###  tips
+ تعداد کانکشن به صورت دیفالت ۱۰۰ تا هست و می توان با این دستور لیست و تعداد سرویس هایی که کانت شده اند را دید .
  
      SELECT * FROM pg_stat_activity;
  + باید بعد از هر کانکشن در سرویس، کلوز کرد .
  + همیشه در سینگلتون کانکشن ، می بایست ریکانکشن هم نوشت . 

###  **Preper statement**

به صورت دیفالت هر بار که درخواست ثبت در دیتا بیس میزنیم تمامی اطلاعات مانند insert into  ارسال میشه ، میتونیم با ننظیم این مفهوم ، دیگه این رو ارسال نکنیم و سرعت رو بیشتر کنیم



## transaction

### tips

**Skip default transaction**

میتونیم موقع کانکشن بگیم نیاز نیست برای هر کوییری ، ترنساکشن بسازی و سرعت بیشتر میشه



## sharding

قبل از شروع باید به چند نکته دقت کرد
+ در شاردینگ برای پرفورمنس ، باید نود ها ایزوله باشند به این معنی که مستقل از هم بتوان کار کرد مثلن کوییری نباشه که رو همه اعمال شه یا جویینی وسطشون باشه

+ هدف توضیع داده است ، نباید در بعضی نود ها داده بیشتر باشد و درست تقسیم نشن
+ باید در همون ابتدا شارد کی رو خیلی دقیق و با دانش انتخاب کنیم

+ احتمال ابنم بدیم که شاید ادمین بخواد کوییری هایی بزنه که جویین داشته باشه بین چند شارد ، اونجا جویین بین چند نود جدا خیلی خیلی پیچیده میشه

+ توجه شود به صورت معمول باید تنها کوییری بر اساس shard key باشد ، و اگر بر اساس دیگر ستون ها باشند ، باید کوییری پارالر زد

###  راه های شارد کرد _Methods for Implementing Sharding in PostgreSQL

+ Manual Sharding:

در این حالت در لایه اپلیکیشن و لاجیک کد این رو هندل میکنیم ، پستگرس نمیداند شارد شده ، همچنین خودمون باید دیتابیس های جدا با کانفیگ های دستی بیاریم بالا

برای مثال چند تا پستگرس رو نود های مختلف بیاریم بالا و شارد کی رو یوزر آی دی بزاریم ، در نهایت در لایه ی اپلیکیشن بدانیم یوزر های فرد در دیتابیس اول هستن و زوج در دومی

+ partition

خود پستگرس قابلیتی داره که میتونه تیبل ها بزرگ رو شارد کنه ، اما به صورت دیفالت روی یک کانکشن و یک سینگل اینستنس انجام میده

Example:CREATE TABLE users (
    id SERIAL PRIMARY KEY,
    user_id INTEGER NOT NULL,
    name TEXT,
    data JSONB
) PARTITION BY HASH (user_id);
   
CREATE TABLE users_p1 PARTITION OF users FOR VALUES WITH (MODULUS 2, REMAINDER 0);
CREATE TABLE users_p2 PARTITION OF users FOR VALUES WITH (MODULUS 2, REMAINDER 1);

+ citus

یه افزونه به پستگرس کهشاردینگ رو هندل میکنه

توجه کنید کوییری ها پارالر هستند

CREATE TABLE users (
    id SERIAL PRIMARY KEY,
    user_id INT NOT NULL,
    name TEXT
) DISTRIBUTED BY (user_id);




##  varchar vs char vs nvarchar
 ابتدا باید دانست که  postgre به صورت دیفالت ، utf8 encoding string  رو فرض میگیره

+  چون varchar  و nvarchar  هر دو variable-length در نتیجه اگر طول رو 255 هم در نظر بگیریم ، باز هم به اندازه مورد نیاز مموری 
اختصاص میده

### varchar

ایده آل برای ذخیره ascii یا utf8 

### nvarchar (nature varchar)

هر دو مقدار_طول ذخیره میکنند اما طراحی شده برای ذخیره unicode data

ایده آل برای متنی که شاید چند زبانه باشد


### منطق و شرط ها در کد یا دیتا بیس؟

اگر منطق در دیتا بیس باشه ، خوبیش اینه که حتی سوپر یوزر هم نمیتونه در دیتابیس به صورت دستی ، داده رو دست کاری کنه

اما هزینه نگهداری و توسعه بیشتره از مزایاش هست ، 

ولی اگر منطق در کد باشه ، سوپر یوزر میتونه بدون توجه به منطق کد ، crud  کنه ، چند راه برای جلو گیری از داستان هست :
+ گاهی یونیک کردن سطر ها سخت تر از شرط های پیش فرض در کانستریت تیبل هست ، مانند چک کردن استتوس بی جای استفاده از getFirst از گرفتن لیستی و در نهایت شرط بزاریم اگر تعداد لیست مخالف یک بود ، بگه



### partial unique index
یکی از خوبی های دیگه ی پستگرس به نسبت mysql داشتن این مورد هست

گاهی ما می خوایم هر یوزر ، یه ایمیل داشته باشه ، پس این دو رو یونیک تو گدر میکنیم ، اما گاهی نیاز داریم هر یوزر تنها یه ایمیل فعال داشته باشه ، در این حالت میتونه چندین ایمیل غیر فعال داشته باشه ، در این صورت از دستور زیر استفاده میکنیم

```sql
CREATE UNIQUE INDEX unique_active_phone_number ON user_phone_numbers (user_id)
WHERE is_active = TRUE;
```



بر خلاف mysql که چندین engine داره مثل innoDb , myisam  ، پستگرس نداره


### ابزار های distribiuted transaction

+ پستگرس به صورت پیش فرض 2pc رو ساپورت میکنه
+ میشه از Postgres-XL/Postgres-XC استفاده کرد ، ابزار هایی با اینترفیس پستگرس هستند ، توجه شود اکستنشن نیست و قابلیت های توضیع پذیری را فراهم میکند مانند sharding , distrib transaction , cross_node query 
توجه شود از 2pc درون خودش استفاده میکنه


### مکانیزم های ذخیره پستگرس

+ Heap Storage:
به صورت دیفالت با این مکانیزم ، استور میکنه ، هر جا که پیدا کنه ذخیره میکنه

+ TOAST (The Oversized-Attribute Storage Technique):
سطر های خیلی بزرگ مانند باینری یا تکست لارج رو یاکامپرس میکنه بعد ذخیره میکنه ، یا در مکان دیگه ذخیره میکنه

+ Unlogged Tables:

داده لاگ میشه ولی ذخیره در همان لحظه نمیشه ، برای زمانی استفاده میشه که سرعت بالا میخواییم ولی داده اهمییت نداره

+ Foreign Data Wrappers (FDW):

اجازه میده دیتابیس های دیگه مثل مای اسکیو ال یا دیگر پستگرس ها دسترسی به دیتا ها ما داشته باشند ، با فرض شناخت تیبل ها
