GC Dynamically Adapting To Application Sizes
Server GC tries to use as much memory as possibleâ€ which is well, not the caseğŸ˜Š. Server GC doesnâ€™t try to use a lot of memory â€” it just by default has the potential to use more memory when there is a lot of memory available. Let me explain why.

One of the most important factors that contribute to more memory usage is the number of heaps. Having more heaps doesnâ€™t automatically mean youâ€™ll use more memory â€” you could just have less memory per heap. And this is the case for long lived data â€” thereâ€™s only so much long-lived data so if you have more heaps each heap will have less long live data. But if you look at it from the user allocation point of view, it means you now have multiple heaps to allocate on instead of one with Workstation GC. Or at least that portion could be N times more if you have N heapsÂ³. And by default, Server GC will have the same number of heaps as the number of logical processors on the machineâ´. This means the problem is not just that your heap may grow to a bigger size. Itâ€™s that itâ€™s unpredictable â€” if you run your workload on a machine with more processors you might see a bigger size.
